{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\leo_b\\OneDrive\\Documentos\\Projects\\personal_projects\\CCC38_2023\\leo.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneighbors\u001b[39;00m \u001b[39mimport\u001b[39;00m KNeighborsClassifier\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "# os\n",
    "import os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Images and Labels\n",
    "def load_images(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "folder = \"train_data\"\n",
    "images = load_images(folder)\n",
    "labels = pd.read_csv(\"train_data/train_data_labels.csv\", header=None).values.ravel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess Images\n",
    "def preprocess_images(images):\n",
    "    processed_images = [cv2.resize(img, (200, 200)).flatten() for img in images]\n",
    "    return np.array(processed_images) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "processed_images = preprocess_images(images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_images, labels, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate KNN Classifier\n",
    "y_pred = knn.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Classification Report:\\n{classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data\n",
      "Mean: 103.6791615\n",
      "Standard Deviation: 69.55526717982558\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_mean_std(image_dir):\n",
    "    pixel_values = []\n",
    "\n",
    "    # Iterate over the image files in the directory\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "            img_path = os.path.join(image_dir, filename)\n",
    "            image = Image.open(img_path)\n",
    "            image = np.array(image)  # Convert the image to a NumPy array\n",
    "            pixel_values.append(image.flatten())\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    pixel_values = np.concatenate(pixel_values, axis=0)\n",
    "    mean = np.mean(pixel_values, axis=0)\n",
    "    std = np.std(pixel_values, axis=0)\n",
    "\n",
    "    print(image_dir)\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Standard Deviation:\", std)\n",
    "\n",
    "image_dir = \"train_data\"\n",
    "get_mean_std(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leo_b\\anaconda3\\envs\\pmba_new\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leo_b\\anaconda3\\envs\\pmba_new\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.8161\n",
      "Epoch [2/20], Loss: 1.5750\n",
      "Epoch [3/20], Loss: 1.4462\n",
      "Epoch [4/20], Loss: 1.3419\n",
      "Epoch [5/20], Loss: 1.2482\n",
      "Epoch [6/20], Loss: 1.2142\n",
      "Epoch [7/20], Loss: 1.1452\n",
      "Epoch [8/20], Loss: 1.0982\n",
      "Epoch [9/20], Loss: 1.0900\n",
      "Epoch [10/20], Loss: 1.0388\n",
      "Epoch [11/20], Loss: 1.0288\n",
      "Epoch [12/20], Loss: 0.9487\n",
      "Epoch [13/20], Loss: 0.9607\n",
      "Epoch [14/20], Loss: 0.9070\n",
      "Epoch [15/20], Loss: 0.8738\n",
      "Epoch [16/20], Loss: 0.8892\n",
      "Epoch [17/20], Loss: 0.8718\n",
      "Epoch [18/20], Loss: 0.8508\n",
      "Epoch [19/20], Loss: 0.8475\n",
      "Epoch [20/20], Loss: 0.8162\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "import csv\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Load the pre-trained model (e.g., ResNet)\n",
    "\n",
    "model_up = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the layers in the model\n",
    "for param in model_up.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final layer of the model to match the number of classes in the new dataset\n",
    "num_ftrs = model_up.fc.in_features\n",
    "model_up.fc = torch.nn.Linear(num_ftrs, 6)  # Adapted to 5 classes\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_up.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_csv, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))])\n",
    "        \n",
    "        # Using csv.reader to read labels from csv file\n",
    "        with open(labels_csv, mode='r') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            self.labels = [int(row[0]) for row in csv_reader]\n",
    "        \n",
    "        if len(self.image_files) != len(self.labels):\n",
    "            raise ValueError(\"Mismatch: Number of image files does not match the number of labels.\")\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "mean = [103.6791615 / 255.0, 103.6791615 / 255.0, 103.6791615 / 255.0]  # Divide by 255 to scale to [0, 1]\n",
    "std = [69.55526717982558 / 255.0, 69.55526717982558 / 255.0, 69.55526717982558 / 255.0]  # Divide by 255 to scale to [0, 1]\n",
    "normalize_transform = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "# Define transformations for the images\n",
    "train_transforms_up = transforms.Compose([\n",
    "    #transforms.Resize((100, 100)),  # adjust size if necessary\n",
    "    transforms.ToTensor(),\n",
    "    normalize_transform\n",
    "])\n",
    "\n",
    "train_data = CustomDataset(image_dir=\"train_data\", labels_csv=\"train_data/train_data_labels.csv\", transform=train_transforms_up)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):  \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_up(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss/len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 0.42\n",
      "[73, 0, 0, 0, 0, 0]\n",
      "[3, 73, 3, 3, 0, 0]\n",
      "[0, 4, 78, 15, 0, 0]\n",
      "[0, 2, 2, 84, 0, 0]\n",
      "[0, 0, 2, 29, 41, 4]\n",
      "[0, 0, 2, 13, 2, 67]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Evaluate the model after training to calculate accuracy and confusion matrix\n",
    "model_up.eval()  # set the model to evaluation mode\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "print(f'Root Mean Square Error (RMSE): {rmse:.2f}')\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model_up(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.numpy().tolist())\n",
    "        all_labels.extend(labels.numpy().tolist())\n",
    "\n",
    "# Calculate accuracy\n",
    "# Calculate RMSE\n",
    "errors = [(pred - actual)**2 for pred, actual in zip(all_preds, all_labels)]\n",
    "rmse = np.sqrt(np.mean(errors))\n",
    "\n",
    "# Compute confusion matrix without sklearn\n",
    "num_classes = 6  # Change this to 6\n",
    "confusion = [[0] * num_classes for _ in range(num_classes)]\n",
    "for actual, pred in zip(all_labels, all_preds):\n",
    "    confusion[actual][pred] += 1\n",
    "\n",
    "for row in confusion:\n",
    "    print(row)\n",
    "\n",
    "# Testing on the test_data folder and outputting the classifications into a txt file\n",
    "\n",
    "# Modify the CustomDataset for image-only mode (no labels)\n",
    "class ImageOnlyDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_name  # return image name too for reference\n",
    "\n",
    "test_data = ImageOnlyDataset(image_dir=\"test_data\", transform=train_transforms_up)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "# Output classifications to a txt file\n",
    "with open(\"classifications.csv\", \"w\") as f:\n",
    "    with torch.no_grad():\n",
    "        for inputs, image_names in test_loader:\n",
    "            outputs = model_up(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            for img, pred in zip(image_names, predicted.numpy()):\n",
    "                f.write(f'{pred}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)  # Modified for regression\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__new__() takes exactly one argument (the type to instantiate)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\leo_b\\OneDrive\\Documentos\\Projects\\personal_projects\\CCC38_2023\\leo.ipynb Cell 14\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#X16sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m image, label\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#X16sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#X16sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     transforms\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#X16sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#X16sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#X16sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m ])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#X16sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m CustomDataset(train_images, train_labels, transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#X16sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m val_dataset \u001b[39m=\u001b[39m CustomDataset(val_images, val_labels, transform\u001b[39m=\u001b[39mtransform)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/leo.ipynb#X16sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\leo_b\\anaconda3\\envs\\pmba_new\\lib\\typing.py:875\u001b[0m, in \u001b[0;36mGeneric.__new__\u001b[1;34m(cls, *args, **kwds)\u001b[0m\n\u001b[0;32m    873\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n\u001b[0;32m    874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 875\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__new__\u001b[39;49m(\u001b[39mcls\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    876\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mTypeError\u001b[0m: object.__new__() takes exactly one argument (the type to instantiate)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import mobilenet_v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "base_path = ''\n",
    "train_data_folder = os.path.join(base_path, 'train_data')\n",
    "train_label_path = os.path.join(base_path, 'train_data_labels.csv')\n",
    "test_data_folder = os.path.join(base_path, 'test_data')\n",
    "\n",
    "with open(train_label_path, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    labels = [int(row[0]) for row in reader]\n",
    "\n",
    "def combine_images(img_path_1, img_path_2):\n",
    "    img1 = np.array(Image.open(img_path_1).convert('RGB'))\n",
    "    img2 = np.array(Image.open(img_path_2).convert('RGB'))\n",
    "    combined_img = np.clip(img1 + img2, 0, 255)\n",
    "    return Image.fromarray(combined_img.astype('uint8'), 'RGB')\n",
    "\n",
    "train_image_filenames = sorted(os.listdir(train_data_folder))\n",
    "train_image_paths = [(os.path.join(train_data_folder, train_image_filenames[i]), os.path.join(train_data_folder, train_image_filenames[i+1])) for i in range(0, len(train_image_filenames)-1, 2) if i+1 < len(train_image_filenames)]\n",
    "train_images = [combine_images(img_pair[0], img_pair[1]) for img_pair in train_image_paths]\n",
    "\n",
    "# Train-validation split\n",
    "split_ratio = 0.8\n",
    "split_idx = int(len(train_images) * split_ratio)\n",
    "combined_list = list(zip(train_images, labels))\n",
    "random.shuffle(combined_list)\n",
    "train_images, labels = zip(*combined_list)\n",
    "train_images, val_images = train_images[:split_idx], train_images[split_idx:]\n",
    "train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def _init_(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def _len_(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(train_images, train_labels, transform=transform)\n",
    "val_dataset = CustomDataset(val_images, val_labels, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=16)\n",
    "\n",
    "model = mobilenet_v2(pretrained=True)\n",
    "model.classifier[1] = torch.nn.Linear(model.last_channel, 1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in val_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images).squeeze(1)\n",
    "    all_predictions.extend(outputs.cpu().numpy())\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "rmse = np.sqrt(((np.array(all_labels) - np.array(all_predictions)) ** 2).mean())\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "\n",
    "model_save_path = os.path.join(base_path, 'mobilenet_v2_trained_model.pth')\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "test_image_filenames = sorted(os.listdir(test_data_folder))\n",
    "test_image_paths = [(os.path.join(test_data_folder, test_image_filenames[i]), os.path.join(test_data_folder, test_image_filenames[i+1])) for i in range(0, len(test_image_filenames), 2)]\n",
    "test_images = [combine_images(img_pair[0], img_pair[1]) for img_pair in test_image_paths]\n",
    "test_dataset = [transform(image) for image in test_images]\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=16)\n",
    "\n",
    "model.eval()\n",
    "predictions_list = []\n",
    "with torch.no_grad():\n",
    "    for image in test_loader:\n",
    "        image = image.to(device)\n",
    "        outputs = model(image).squeeze(1)\n",
    "        predictions_list.extend(np.round(outputs.cpu().numpy()))\n",
    "\n",
    "csv_output_path = os.path.join(base_path, \"test_predictions.csv\")\n",
    "with open(csv_output_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for pred in predictions_list:\n",
    "        writer.writerow([int(pred)])\n",
    "\n",
    "print(f\"Predictions saved in {csv_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
