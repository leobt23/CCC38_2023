{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.6.0\" to C:\\Users\\leo_b/.cache\\torch\\hub\\v0.6.0.zip\n",
      "c:\\Users\\leo_b\\anaconda3\\envs\\CCC\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leo_b\\anaconda3\\envs\\CCC\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\leo_b\\anaconda3\\envs\\CCC\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1])) that is different to the input size (torch.Size([16, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 12162.2582\n",
      "Epoch 2/10 - Loss: 11971.6738\n",
      "Epoch 3/10 - Loss: 11742.2716\n",
      "Epoch 4/10 - Loss: 11400.5501\n",
      "Epoch 5/10 - Loss: 10953.1561\n",
      "Epoch 6/10 - Loss: 10534.6061\n",
      "Epoch 7/10 - Loss: 10177.7531\n",
      "Epoch 8/10 - Loss: 9886.1740\n",
      "Epoch 9/10 - Loss: 9584.3437\n",
      "Epoch 10/10 - Loss: 9323.1285\n",
      "Mean Squared Euclidean Distance on validation set: 17721.2734375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Define path variables\n",
    "base_path = \"C:\\\\Users\\\\leo_b\\\\OneDrive\\\\Documentos\\\\Projects\\\\personal_projects\\\\CCC38_2023\\\\\"\n",
    "\n",
    "train_data_folder = os.path.join(base_path, 'train_data')\n",
    "train_label_path = os.path.join(base_path, 'train_data_labels.csv')  # Assuming CSV has 'x,y' for each image pair\n",
    "test_data_folder = os.path.join(base_path, 'test_data')\n",
    "\n",
    "# Combine the two cloudy images into a single clearer image\n",
    "def combine_images(img_path1, img_path2):\n",
    "    img1 = np.array(Image.open(img_path1).convert('RGB'))\n",
    "    img2 = np.array(Image.open(img_path2).convert('RGB'))\n",
    "    combined_img = np.clip(img1 + img2, 0, 255)\n",
    "    return Image.fromarray(combined_img.astype('uint8'), 'RGB')\n",
    "\n",
    "# Load the training data\n",
    "with open(train_label_path, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    labels = [list(map(int, row[0].split(','))) for row in reader]\n",
    "\n",
    "# Process training images\n",
    "train_image_filenames = sorted(os.listdir(train_data_folder))\n",
    "train_image_paths = [(os.path.join(train_data_folder, train_image_filenames[i]), os.path.join(train_data_folder, train_image_filenames[i+1])) for i in range(0, len(train_image_filenames), 2)]\n",
    "train_images = [combine_images(img_pair[0], img_pair[1]) for img_pair in train_image_paths]\n",
    "\n",
    "# Data loading\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split data into training and validation (80-20 split)\n",
    "split_idx = int(0.8 * len(train_images))\n",
    "train_dataset = CustomDataset(train_images[:split_idx], labels[:split_idx], transform=transform)\n",
    "val_dataset = CustomDataset(train_images[split_idx:], labels[split_idx:], transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=16)\n",
    "\n",
    "# Model (using MobileNetV2 as a regression model)\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'mobilenet_v2', pretrained=True)\n",
    "model.classifier[1] = torch.nn.Linear(model.last_channel, 2)  # Predicting two values, x and y coordinates\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 60\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluation on validation set\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "for images, labels in val_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    all_predictions.append(outputs.cpu().numpy())\n",
    "    all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_predictions = np.vstack(all_predictions)\n",
    "all_labels = np.vstack(all_labels)\n",
    "mse_distance = np.mean(np.sum((all_predictions - all_labels)**2, axis=1))\n",
    "print(f\"Mean Squared Euclidean Distance on validation set: {mse_distance}\")\n",
    "\n",
    "# If this is within acceptable limits, you can then proceed to generate predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Process test images\n",
    "test_image_filenames = sorted(os.listdir(test_data_folder))\n",
    "test_image_paths = [(os.path.join(test_data_folder, test_image_filenames[i]), os.path.join(test_data_folder, test_image_filenames[i+1])) for i in range(0, len(test_image_filenames), 2)]\n",
    "test_images = [combine_images(img_pair[0], img_pair[1]) for img_pair in test_image_paths]\n",
    "\n",
    "# Create a DataLoader for test images\n",
    "test_dataset = CustomDataset(test_images, [0]*len(test_images), transform=transform)  # labels are not used\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=16)\n",
    "\n",
    "# Predict on test set\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "for images, _ in test_loader:  # we don't need labels for test data\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    test_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "test_predictions = np.vstack(test_predictions)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "with open('test_predictions.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['x', 'y'])\n",
    "    writer.writerows(test_predictions)\n",
    "\n",
    "print(\"Test predictions saved to test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 500 500\n"
     ]
    }
   ],
   "source": [
    "print(len(labels), len(train_images), len(train_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['67', '140'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leo_b\\anaconda3\\envs\\CCC\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leo_b\\anaconda3\\envs\\CCC\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training Loss: 11932.4604, Validation Loss: 12302.6286\n",
      "Epoch 2/10 - Training Loss: 11762.5370, Validation Loss: 12133.9271\n",
      "Epoch 3/10 - Training Loss: 11590.9665, Validation Loss: 12011.3538\n",
      "Epoch 4/10 - Training Loss: 11423.7551, Validation Loss: 11825.2613\n",
      "Epoch 5/10 - Training Loss: 11236.3523, Validation Loss: 11574.1909\n",
      "Epoch 6/10 - Training Loss: 11038.6578, Validation Loss: 11507.8681\n",
      "Epoch 7/10 - Training Loss: 10828.7107, Validation Loss: 11203.0730\n",
      "Epoch 8/10 - Training Loss: 10606.0601, Validation Loss: 11066.4394\n",
      "Epoch 9/10 - Training Loss: 10388.6463, Validation Loss: 11142.0090\n",
      "Epoch 10/10 - Training Loss: 10170.5149, Validation Loss: 10598.9123\n",
      "Finished Training\n",
      "Mean Squared Euclidean Distance on Validation set: 21197.6589\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import mobilenet_v2\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define path variables\n",
    "base_path = \"C:\\\\Users\\\\leo_b\\\\OneDrive\\\\Documentos\\\\Projects\\\\personal_projects\\\\CCC38_2023\\\\\"\n",
    "\n",
    "train_data_folder = os.path.join(base_path, 'train_data')\n",
    "train_label_path = os.path.join(base_path, 'train_data_labels.csv')  # Assuming CSV has 'x,y' for each image pair\n",
    "test_data_folder = os.path.join(base_path, 'test_data')\n",
    "\n",
    "\n",
    "\n",
    "# Load training labels (assuming CSV has 'x', 'y' columns)\n",
    "df_labels = pd.read_csv(train_label_path)\n",
    "print(df_labels.columns)\n",
    "df_labels.columns = ['x', 'y']\n",
    "labels = df_labels[['x', 'y']].values\n",
    "labels = df_labels[['x', 'y']].values\n",
    "\n",
    "# Combine images and pre-process\n",
    "def combine_images(path1, path2):\n",
    "    img1 = Image.open(path1).convert('RGB')\n",
    "    img2 = Image.open(path2).convert('RGB')\n",
    "    \n",
    "    combined_img = np.maximum(np.array(img1), np.array(img2))\n",
    "    return Image.fromarray(combined_img.astype('uint8'), 'RGB')\n",
    "\n",
    "train_image_filenames = sorted(os.listdir(train_data_folder))\n",
    "train_image_paths = [(os.path.join(train_data_folder, train_image_filenames[i]), os.path.join(train_data_folder, train_image_filenames[i+1])) for i in range(0, len(train_image_filenames)-1, 2)]\n",
    "train_images = [combine_images(img_pair[0], img_pair[1]) for img_pair in train_image_paths]\n",
    "\n",
    "# Custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx-1]\n",
    "        label = torch.tensor(self.labels[idx-1], dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = CustomDataset(train_images, labels, transform=transform)\n",
    "\n",
    "# Splitting data into train and validation subsets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=16)\n",
    "\n",
    "# Model: Using MobileNetV2 and adapting last layer for regression\n",
    "model = mobilenet_v2(pretrained=True)\n",
    "model.classifier[1] = torch.nn.Linear(model.last_channel, 2)  # 2 output neurons for x, y coordinates\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = torch.nn.MSELoss()  # Regression task, hence MSE\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 10\n",
    "\n",
    "# Training the model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, coords in train_loader:\n",
    "            images, coords = images.to(device), coords.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, coords)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validate on the validation set\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, coords in val_loader:\n",
    "                images, coords = images.to(device), coords.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, coords)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {epoch_loss:.4f}, Validation Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# Evaluate the performance\n",
    "def compute_mean_squared_euclidean_distance(pred, true):\n",
    "    return ((pred - true) ** 2).sum(1).mean().item()\n",
    "\n",
    "model.eval()\n",
    "total_msed = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, coords in val_loader:\n",
    "        images, coords = images.to(device), coords.to(device)\n",
    "        outputs = model(images)\n",
    "        msed = compute_mean_squared_euclidean_distance(outputs, coords)\n",
    "        total_msed += msed * images.size(0)\n",
    "\n",
    "avg_msed = total_msed / len(val_loader.dataset)\n",
    "print(f\"Mean Squared Euclidean Distance on Validation set: {avg_msed:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 499 is out of bounds for axis 0 with size 499",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\leo_b\\OneDrive\\Documentos\\Projects\\personal_projects\\CCC38_2023\\lopes.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m - Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Validation Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_epoch_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished Training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs\u001b[39m=\u001b[39;49mnum_epochs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Evaluate the performance\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_mean_squared_euclidean_distance\u001b[39m(pred, true):\n",
      "\u001b[1;32mc:\\Users\\leo_b\\OneDrive\\Documentos\\Projects\\personal_projects\\CCC38_2023\\lopes.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()  \u001b[39m# Set model to training mode\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, coords \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     images, coords \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), coords\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leo_b\\anaconda3\\envs\\CCC\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\leo_b\\anaconda3\\envs\\CCC\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\leo_b\\anaconda3\\envs\\CCC\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\leo_b\\anaconda3\\envs\\CCC\\lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\leo_b\\anaconda3\\envs\\CCC\\lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "\u001b[1;32mc:\\Users\\leo_b\\OneDrive\\Documentos\\Projects\\personal_projects\\CCC38_2023\\lopes.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages[idx]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels[idx], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leo_b/OneDrive/Documentos/Projects/personal_projects/CCC38_2023/lopes.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(image)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 499 is out of bounds for axis 0 with size 499"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine and preprocess test images\n",
    "test_image_filenames = sorted(os.listdir(test_data_folder))\n",
    "test_image_paths = [(os.path.join(test_data_folder, test_image_filenames[i]), os.path.join(test_data_folder, test_image_filenames[i+1])) for i in range(0, len(test_image_filenames)-1, 2)]\n",
    "test_images = [combine_images(img_pair[0], img_pair[1]) for img_pair in test_image_paths]\n",
    "\n",
    "# Using the same custom dataset but without labels for test images\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "test_dataset = TestDataset(test_images, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=16)\n",
    "\n",
    "# Get predictions on the test set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        test_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "# Flatten list of arrays into one array\n",
    "test_predictions = np.concatenate(test_predictions, axis=0)\n",
    "\n",
    "# Pass test_predictions to int with no decimal cases\n",
    "test_predictions = test_predictions.astype(int)\n",
    "\n",
    "# Pass negatives to positives\n",
    "test_predictions[test_predictions < 0] = 1\n",
    "\n",
    "# Optionally, save predictions to a CSV file\n",
    "save_path = os.path.join(base_path, 'test_predictions.csv')\n",
    "pd.DataFrame(test_predictions, columns=['x', 'y']).to_csv(save_path, index=False)\n",
    "\n",
    "print(\"Test predictions saved to test_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CCC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
